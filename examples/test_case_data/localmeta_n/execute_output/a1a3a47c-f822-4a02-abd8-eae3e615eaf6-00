{
  "uuid" : "a1a3a47c-f822-4a02-abd8-eae3e615eaf6-00",
  "last_modified" : 1576667684300,
  "version" : "3.0.0.20500",
  "content" : "java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat io.kyligence.kap.engine.spark.job.NSparkExecutable.runLocalMode(NSparkExecutable.java:359)\n\tat io.kyligence.kap.engine.spark.job.NSparkExecutable.doWork(NSparkExecutable.java:153)\n\tat org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:190)\n\tat org.apache.kylin.job.execution.DefaultChainedExecutable.doWork(DefaultChainedExecutable.java:76)\n\tat org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:190)\n\tat org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:114)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.RuntimeException: Error execute io.kyligence.kap.engine.spark.job.ResourceDetectBeforeCubingJob\n\tat io.kyligence.kap.engine.spark.application.SparkApplication.execute(SparkApplication.java:89)\n\tat io.kyligence.kap.engine.spark.job.ResourceDetectBeforeCubingJob.main(ResourceDetectBeforeCubingJob.java:99)\n\t... 13 more\nCaused by: org.apache.spark.sql.AnalysisException: cannot resolve '`SELLER_COUNTRY_0_DOT_0_NAME`' given input columns: [TEST_KYLIN_FACT_0_DOT_0_SELLER_ID, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_CONTACT, TEST_KYLIN_FACT_0_DOT_0_ORDER_ID, TEST_KYLIN_FACT_0_DOT_0_TEST_COUNT_DISTINCT_BITMAP, TEST_KYLIN_FACT_0_DOT_0_LSTG_SITE_ID, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_SELLER_LEVEL, TEST_KYLIN_FACT_0_DOT_0_LEAF_CATEG_ID, TEST_KYLIN_FACT_0_DOT_0_TEST_COUNT_COLUMN, TEST_KYLIN_FACT_0_DOT_0_SLR_SEGMENT_CD, TEST_KYLIN_FACT_0_DOT_0_PRICE, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_BUYER_LEVEL, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_BUYER_LEVEL, TEST_ORDER_0_DOT_0_ORDER_ID, TEST_KYLIN_FACT_0_DOT_0_CAL_DT, TEST_ORDER_0_DOT_0_TEST_DATE_ENC, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_ID, TEST_ORDER_0_DOT_0_TEST_TIME_ENC, TEST_KYLIN_FACT_0_DOT_0_LSTG_FORMAT_NAME, TEST_ORDER_0_DOT_0_TEST_EXTENDED_COLUMN, TEST_KYLIN_FACT_0_DOT_0_ITEM_COUNT, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_COUNTRY, TEST_ORDER_0_DOT_0_BUYER_ID, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_SELLER_LEVEL, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_ID, TEST_KYLIN_FACT_0_DOT_0_TRANS_ID, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_COUNTRY, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_CONTACT]; line 1 pos 0;\n'Project ['SELLER_COUNTRY_0_DOT_0_NAME AS 0#333, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_COUNTRY#123 AS 5#334, TEST_ORDER_0_DOT_0_TEST_TIME_ENC#73 AS 10#335, 'TEST_CATEGORY_GROUPINGS_0_DOT_0_UPD_USER AS 42#336, 'TEST_CAL_DT_0_DOT_0_WEEK_BEG_DT AS 24#337, TEST_ORDER_0_DOT_0_TEST_EXTENDED_COLUMN#74 AS 37#338, 'TEST_SELLER_TYPE_DIM_0_DOT_0_SELLER_TYPE_CD AS 25#339, 'TEST_CATEGORY_GROUPINGS_0_DOT_0_CATEG_LVL3_NAME AS 14#340, TEST_KYLIN_FACT_0_DOT_0_SELLER_ID#31 AS 20#341, 'TEST_SITES_0_DOT_0_SITE_NAME AS 46#342, 'SELLER_COUNTRY_0_DOT_0_COUNTRY AS 29#343, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_COUNTRY#98 AS 1#344, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_SELLER_LEVEL#122 AS 6#345, TEST_KYLIN_FACT_0_DOT_0_TRANS_ID#24L AS 28#346L, 'TEST_CATEGORY_GROUPINGS_0_DOT_0_LEAF_CATEG_ID AS 38#347, TEST_KYLIN_FACT_0_DOT_0_TEST_COUNT_COLUMN#35 AS 21#348, TEST_KYLIN_FACT_0_DOT_0_ITEM_COUNT#33 AS 33#349, TEST_ORDER_0_DOT_0_TEST_DATE_ENC#72 AS 9#350, TEST_KYLIN_FACT_0_DOT_0_LSTG_FORMAT_NAME#27 AS 13#351, TEST_KYLIN_FACT_0_DOT_0_TEST_COUNT_DISTINCT_BITMAP#34 AS 41#352, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_SELLER_LEVEL#97 AS 2#353, TEST_KYLIN_FACT_0_DOT_0_PRICE#32 AS 32#354, 'TEST_SELLER_TYPE_DIM_0_DOT_0_SELLER_TYPE_DESC AS 34#355, 'TEST_SITES_0_DOT_0_CRE_USER AS 45#356, ... 23 more fields]\n+- Join Inner, (TEST_ORDER_0_DOT_0_BUYER_ID#71L = BUYER_ACCOUNT_0_DOT_0_ACCOUNT_ID#120L)\n   :- Join Inner, (cast(TEST_KYLIN_FACT_0_DOT_0_SELLER_ID#31 as bigint) = SELLER_ACCOUNT_0_DOT_0_ACCOUNT_ID#95L)\n   :  :- Join Inner, (TEST_KYLIN_FACT_0_DOT_0_ORDER_ID#25L = TEST_ORDER_0_DOT_0_ORDER_ID#70L)\n   :  :  :- Project [TEST_KYLIN_FACT_0_DOT_0_TRANS_ID#24L, TEST_KYLIN_FACT_0_DOT_0_ORDER_ID#25L, TEST_KYLIN_FACT_0_DOT_0_CAL_DT#26, TEST_KYLIN_FACT_0_DOT_0_LSTG_FORMAT_NAME#27, TEST_KYLIN_FACT_0_DOT_0_LEAF_CATEG_ID#28L, TEST_KYLIN_FACT_0_DOT_0_LSTG_SITE_ID#29, TEST_KYLIN_FACT_0_DOT_0_SLR_SEGMENT_CD#30, TEST_KYLIN_FACT_0_DOT_0_SELLER_ID#31, TEST_KYLIN_FACT_0_DOT_0_PRICE#32, TEST_KYLIN_FACT_0_DOT_0_ITEM_COUNT#33, TEST_KYLIN_FACT_0_DOT_0_TEST_COUNT_DISTINCT_BITMAP#34, TEST_KYLIN_FACT_0_DOT_0_TEST_COUNT_COLUMN#35]\n   :  :  :  +- Project [TRANS_ID#0L AS TEST_KYLIN_FACT_0_DOT_0_TRANS_ID#24L, ORDER_ID#1L AS TEST_KYLIN_FACT_0_DOT_0_ORDER_ID#25L, CAL_DT#2 AS TEST_KYLIN_FACT_0_DOT_0_CAL_DT#26, LSTG_FORMAT_NAME#3 AS TEST_KYLIN_FACT_0_DOT_0_LSTG_FORMAT_NAME#27, LEAF_CATEG_ID#4L AS TEST_KYLIN_FACT_0_DOT_0_LEAF_CATEG_ID#28L, LSTG_SITE_ID#5 AS TEST_KYLIN_FACT_0_DOT_0_LSTG_SITE_ID#29, SLR_SEGMENT_CD#6 AS TEST_KYLIN_FACT_0_DOT_0_SLR_SEGMENT_CD#30, SELLER_ID#7 AS TEST_KYLIN_FACT_0_DOT_0_SELLER_ID#31, PRICE#8 AS TEST_KYLIN_FACT_0_DOT_0_PRICE#32, ITEM_COUNT#9 AS TEST_KYLIN_FACT_0_DOT_0_ITEM_COUNT#33, TEST_COUNT_DISTINCT_BITMAP#10 AS TEST_KYLIN_FACT_0_DOT_0_TEST_COUNT_DISTINCT_BITMAP#34, TEST_COUNT_COLUMN#11 AS TEST_KYLIN_FACT_0_DOT_0_TEST_COUNT_COLUMN#35]\n   :  :  :     +- SubqueryAlias `TEST_KYLIN_FACT`\n   :  :  :        +- Relation[TRANS_ID#0L,ORDER_ID#1L,CAL_DT#2,LSTG_FORMAT_NAME#3,LEAF_CATEG_ID#4L,LSTG_SITE_ID#5,SLR_SEGMENT_CD#6,SELLER_ID#7,PRICE#8,ITEM_COUNT#9,TEST_COUNT_DISTINCT_BITMAP#10,TEST_COUNT_COLUMN#11] csv\n   :  :  +- Project [TEST_ORDER_0_DOT_0_ORDER_ID#70L, TEST_ORDER_0_DOT_0_BUYER_ID#71L, TEST_ORDER_0_DOT_0_TEST_DATE_ENC#72, TEST_ORDER_0_DOT_0_TEST_TIME_ENC#73, TEST_ORDER_0_DOT_0_TEST_EXTENDED_COLUMN#74]\n   :  :     +- Project [ORDER_ID#60L AS TEST_ORDER_0_DOT_0_ORDER_ID#70L, BUYER_ID#61L AS TEST_ORDER_0_DOT_0_BUYER_ID#71L, TEST_DATE_ENC#62 AS TEST_ORDER_0_DOT_0_TEST_DATE_ENC#72, TEST_TIME_ENC#63 AS TEST_ORDER_0_DOT_0_TEST_TIME_ENC#73, TEST_EXTENDED_COLUMN#64 AS TEST_ORDER_0_DOT_0_TEST_EXTENDED_COLUMN#74]\n   :  :        +- SubqueryAlias `TEST_ORDER`\n   :  :           +- Relation[ORDER_ID#60L,BUYER_ID#61L,TEST_DATE_ENC#62,TEST_TIME_ENC#63,TEST_EXTENDED_COLUMN#64] csv\n   :  +- Project [SELLER_ACCOUNT_0_DOT_0_ACCOUNT_ID#95L, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_BUYER_LEVEL#96, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_SELLER_LEVEL#97, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_COUNTRY#98, SELLER_ACCOUNT_0_DOT_0_ACCOUNT_CONTACT#99]\n   :     +- Project [ACCOUNT_ID#85L AS SELLER_ACCOUNT_0_DOT_0_ACCOUNT_ID#95L, ACCOUNT_BUYER_LEVEL#86 AS SELLER_ACCOUNT_0_DOT_0_ACCOUNT_BUYER_LEVEL#96, ACCOUNT_SELLER_LEVEL#87 AS SELLER_ACCOUNT_0_DOT_0_ACCOUNT_SELLER_LEVEL#97, ACCOUNT_COUNTRY#88 AS SELLER_ACCOUNT_0_DOT_0_ACCOUNT_COUNTRY#98, ACCOUNT_CONTACT#89 AS SELLER_ACCOUNT_0_DOT_0_ACCOUNT_CONTACT#99]\n   :        +- SubqueryAlias `SELLER_ACCOUNT`\n   :           +- Relation[ACCOUNT_ID#85L,ACCOUNT_BUYER_LEVEL#86,ACCOUNT_SELLER_LEVEL#87,ACCOUNT_COUNTRY#88,ACCOUNT_CONTACT#89] csv\n   +- Project [BUYER_ACCOUNT_0_DOT_0_ACCOUNT_ID#120L, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_BUYER_LEVEL#121, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_SELLER_LEVEL#122, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_COUNTRY#123, BUYER_ACCOUNT_0_DOT_0_ACCOUNT_CONTACT#124]\n      +- Project [ACCOUNT_ID#110L AS BUYER_ACCOUNT_0_DOT_0_ACCOUNT_ID#120L, ACCOUNT_BUYER_LEVEL#111 AS BUYER_ACCOUNT_0_DOT_0_ACCOUNT_BUYER_LEVEL#121, ACCOUNT_SELLER_LEVEL#112 AS BUYER_ACCOUNT_0_DOT_0_ACCOUNT_SELLER_LEVEL#122, ACCOUNT_COUNTRY#113 AS BUYER_ACCOUNT_0_DOT_0_ACCOUNT_COUNTRY#123, ACCOUNT_CONTACT#114 AS BUYER_ACCOUNT_0_DOT_0_ACCOUNT_CONTACT#124]\n         +- SubqueryAlias `BUYER_ACCOUNT`\n            +- Relation[ACCOUNT_ID#110L,ACCOUNT_BUYER_LEVEL#111,ACCOUNT_SELLER_LEVEL#112,ACCOUNT_COUNTRY#113,ACCOUNT_CONTACT#114] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.immutable.List.map(List.scala:285)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3410)\n\tat org.apache.spark.sql.Dataset.select(Dataset.scala:1334)\n\tat io.kyligence.kap.engine.spark.builder.CreateFlatTable$.changeSchemeToColumnIndice(CreateFlatTable.scala:214)\n\tat io.kyligence.kap.engine.spark.builder.CreateFlatTable.generateDataset(CreateFlatTable.scala:75)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser.getFlatTable(ParentSourceChooser.scala:163)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser.io$kyligence$kap$engine$spark$job$ParentSourceChooser$$decideFlatTableSource(ParentSourceChooser.scala:73)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser$$anonfun$decideSources$1.apply(ParentSourceChooser.scala:59)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser$$anonfun$decideSources$1.apply(ParentSourceChooser.scala:54)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser.decideSources(ParentSourceChooser.scala:54)\n\tat io.kyligence.kap.engine.spark.job.ResourceDetectBeforeCubingJob.doExecute(ResourceDetectBeforeCubingJob.java:60)\n\tat io.kyligence.kap.engine.spark.application.SparkApplication.execute(SparkApplication.java:266)\n\tat io.kyligence.kap.engine.spark.application.SparkApplication.execute(SparkApplication.java:86)\n\t... 14 more\n",
  "status" : "ERROR",
  "info" : {
    "startTime" : "1576667127863",
    "endTime" : "1576667684291"
  }
}