{
  "uuid" : "0188b18e-e511-4ef0-beac-6d65762e5a07",
  "last_modified" : 1576683807236,
  "version" : "3.0.0.20500",
  "content" : "org.apache.kylin.job.exception.ExecuteException: java.lang.reflect.InvocationTargetException\n\tat org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:205)\n\tat org.apache.kylin.job.execution.DefaultChainedExecutable.doWork(DefaultChainedExecutable.java:76)\n\tat org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:190)\n\tat org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:114)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat io.kyligence.kap.engine.spark.job.NSparkExecutable.runLocalMode(NSparkExecutable.java:359)\n\tat io.kyligence.kap.engine.spark.job.NSparkExecutable.doWork(NSparkExecutable.java:153)\n\tat org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:190)\n\t... 6 more\nCaused by: java.lang.RuntimeException: Error execute io.kyligence.kap.engine.spark.job.ResourceDetectBeforeCubingJob\n\tat io.kyligence.kap.engine.spark.application.SparkApplication.execute(SparkApplication.java:89)\n\tat io.kyligence.kap.engine.spark.job.ResourceDetectBeforeCubingJob.main(ResourceDetectBeforeCubingJob.java:99)\n\t... 13 more\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.NoSuchMethodError: org.apache.commons.lang3.time.FastDateFormat.parse(Ljava/lang/String;)Ljava/util/Date;\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14$$anonfun$apply$3.apply$mcI$sp(UnivocityParser.scala:173)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14$$anonfun$apply$3.apply(UnivocityParser.scala:173)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14$$anonfun$apply$3.apply(UnivocityParser.scala:173)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14.apply(UnivocityParser.scala:173)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14.apply(UnivocityParser.scala:170)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$nullSafeDatum(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10.apply(UnivocityParser.scala:170)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10.apply(UnivocityParser.scala:169)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(UnivocityParser.scala:252)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$getPartialResult$1(UnivocityParser.scala:235)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$4.apply(UnivocityParser.scala:244)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$4.apply(UnivocityParser.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:73)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$parseIterator$1.apply(UnivocityParser.scala:344)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$parseIterator$1.apply(UnivocityParser.scala:344)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:622)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1363)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2064)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2085)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2104)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2129)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3387)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2798)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectAsList$1.apply(Dataset.scala:2797)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3368)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$2.apply(SQLExecution.scala:87)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3367)\n\tat org.apache.spark.sql.Dataset.collectAsList(Dataset.scala:2797)\n\tat io.kyligence.kap.engine.spark.mockup.CsvSource$1.getSourceData(CsvSource.java:47)\n\tat io.kyligence.kap.engine.spark.utils.SparkDataSource$SparkSource.table(SparkDataSource.scala:33)\n\tat io.kyligence.kap.engine.spark.builder.CreateFlatTable$.io$kyligence$kap$engine$spark$builder$CreateFlatTable$$generateTableDataset(CreateFlatTable.scala:124)\n\tat io.kyligence.kap.engine.spark.builder.CreateFlatTable.generateDataset(CreateFlatTable.scala:47)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser.getFlatTable(ParentSourceChooser.scala:163)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser.io$kyligence$kap$engine$spark$job$ParentSourceChooser$$decideFlatTableSource(ParentSourceChooser.scala:73)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser$$anonfun$decideSources$1.apply(ParentSourceChooser.scala:59)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser$$anonfun$decideSources$1.apply(ParentSourceChooser.scala:54)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser.decideSources(ParentSourceChooser.scala:54)\n\tat io.kyligence.kap.engine.spark.job.ResourceDetectBeforeCubingJob.doExecute(ResourceDetectBeforeCubingJob.java:60)\n\tat io.kyligence.kap.engine.spark.application.SparkApplication.execute(SparkApplication.java:266)\n\tat io.kyligence.kap.engine.spark.application.SparkApplication.execute(SparkApplication.java:86)\n\t... 14 more\nCaused by: java.lang.NoSuchMethodError: org.apache.commons.lang3.time.FastDateFormat.parse(Ljava/lang/String;)Ljava/util/Date;\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14$$anonfun$apply$3.apply$mcI$sp(UnivocityParser.scala:173)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14$$anonfun$apply$3.apply(UnivocityParser.scala:173)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14$$anonfun$apply$3.apply(UnivocityParser.scala:173)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14.apply(UnivocityParser.scala:173)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10$$anonfun$apply$14.apply(UnivocityParser.scala:170)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$nullSafeDatum(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10.apply(UnivocityParser.scala:170)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$makeConverter$10.apply(UnivocityParser.scala:169)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(UnivocityParser.scala:252)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser.org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$getPartialResult$1(UnivocityParser.scala:235)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$4.apply(UnivocityParser.scala:244)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$4.apply(UnivocityParser.scala:244)\n\tat org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:73)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$parseIterator$1.apply(UnivocityParser.scala:344)\n\tat org.apache.spark.sql.execution.datasources.csv.UnivocityParser$$anonfun$parseIterator$1.apply(UnivocityParser.scala:344)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:181)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:622)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1363)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\t... 3 more\n",
  "status" : "ERROR",
  "info" : {
    "startTime" : "1576683803368",
    "buildInstance" : "5769@yimingxumac.local",
    "endTime" : "1576683807235"
  }
}