{
  "uuid" : "0876a13e-a6ed-40c1-8f72-c97452cbbfd8",
  "last_modified" : 1576682598176,
  "version" : "3.0.0.20500",
  "content" : "org.apache.kylin.job.exception.ExecuteException: java.lang.reflect.InvocationTargetException\n\tat org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:205)\n\tat org.apache.kylin.job.execution.DefaultChainedExecutable.doWork(DefaultChainedExecutable.java:76)\n\tat org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:190)\n\tat org.apache.kylin.job.impl.threadpool.DefaultScheduler$JobRunner.run(DefaultScheduler.java:114)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat io.kyligence.kap.engine.spark.job.NSparkExecutable.runLocalMode(NSparkExecutable.java:359)\n\tat io.kyligence.kap.engine.spark.job.NSparkExecutable.doWork(NSparkExecutable.java:153)\n\tat org.apache.kylin.job.execution.AbstractExecutable.execute(AbstractExecutable.java:190)\n\t... 6 more\nCaused by: java.lang.RuntimeException: Error execute io.kyligence.kap.engine.spark.job.ResourceDetectBeforeCubingJob\n\tat io.kyligence.kap.engine.spark.application.SparkApplication.execute(SparkApplication.java:89)\n\tat io.kyligence.kap.engine.spark.job.ResourceDetectBeforeCubingJob.main(ResourceDetectBeforeCubingJob.java:99)\n\t... 13 more\nCaused by: java.lang.NullPointerException\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$14.apply(SQLConf.scala:133)\n\tat org.apache.spark.sql.internal.SQLConf$$anonfun$14.apply(SQLConf.scala:133)\n\tat scala.Option.map(Option.scala:146)\n\tat org.apache.spark.sql.internal.SQLConf$.get(SQLConf.scala:133)\n\tat org.apache.spark.sql.types.DataType.sameType(DataType.scala:88)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryOperator.checkInputDataTypes(Expression.scala:581)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryComparison.checkInputDataTypes(predicates.scala:549)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:169)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:169)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join$$anonfun$resolvedExceptNatural$1.apply(basicLogicalOperators.scala:341)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join$$anonfun$resolvedExceptNatural$1.apply(basicLogicalOperators.scala:341)\n\tat scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.scala:83)\n\tat scala.collection.immutable.Stream.forall(Stream.scala:202)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.resolvedExceptNatural$lzycompute(basicLogicalOperators.scala:341)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.resolvedExceptNatural(basicLogicalOperators.scala:342)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.resolved$lzycompute(basicLogicalOperators.scala:351)\n\tat org.apache.spark.sql.catalyst.plans.logical.Join.resolved(basicLogicalOperators.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$36.applyOrElse(Analyzer.scala:2385)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$$anonfun$apply$36.applyOrElse(Analyzer.scala:2383)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2383)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveDeserializer$.apply(Analyzer.scala:2382)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3410)\n\tat org.apache.spark.sql.Dataset.join(Dataset.scala:995)\n\tat io.kyligence.kap.engine.spark.builder.CreateFlatTable$.joinTableDataset(CreateFlatTable.scala:191)\n\tat io.kyligence.kap.engine.spark.builder.CreateFlatTable$$anonfun$joinFactTableWithLookupTables$1.apply(CreateFlatTable.scala:165)\n\tat io.kyligence.kap.engine.spark.builder.CreateFlatTable$$anonfun$joinFactTableWithLookupTables$1.apply(CreateFlatTable.scala:164)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)\n\tat scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:221)\n\tat scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:428)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)\n\tat io.kyligence.kap.engine.spark.builder.CreateFlatTable$.joinFactTableWithLookupTables(CreateFlatTable.scala:163)\n\tat io.kyligence.kap.engine.spark.builder.CreateFlatTable.generateDataset(CreateFlatTable.scala:68)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser.getFlatTable(ParentSourceChooser.scala:163)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser.io$kyligence$kap$engine$spark$job$ParentSourceChooser$$decideFlatTableSource(ParentSourceChooser.scala:73)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser$$anonfun$decideSources$1.apply(ParentSourceChooser.scala:59)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser$$anonfun$decideSources$1.apply(ParentSourceChooser.scala:54)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat io.kyligence.kap.engine.spark.job.ParentSourceChooser.decideSources(ParentSourceChooser.scala:54)\n\tat io.kyligence.kap.engine.spark.job.ResourceDetectBeforeCubingJob.doExecute(ResourceDetectBeforeCubingJob.java:60)\n\tat io.kyligence.kap.engine.spark.application.SparkApplication.execute(SparkApplication.java:266)\n\tat io.kyligence.kap.engine.spark.application.SparkApplication.execute(SparkApplication.java:86)\n\t... 14 more\n",
  "status" : "ERROR",
  "info" : {
    "startTime" : "1576682466468",
    "buildInstance" : "4670@yimingxumac.local",
    "endTime" : "1576682598174"
  }
}